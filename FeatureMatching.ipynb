{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from typing import List, Dict\n",
    "from kornia.feature import DenseSIFTDescriptor\n",
    "from kornia.filters import spatial_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG-16 Layer Names and Channels\n",
    "vgg16_layers = {\n",
    "    \"conv1_1\": 64,\n",
    "    \"relu1_1\": 64,\n",
    "    \"conv1_2\": 64,\n",
    "    \"relu1_2\": 64,\n",
    "    \"pool1\": 64,\n",
    "    \"conv2_1\": 128,\n",
    "    \"relu2_1\": 128,\n",
    "    \"conv2_2\": 128,\n",
    "    \"relu2_2\": 128,\n",
    "    \"pool2\": 128,\n",
    "    \"conv3_1\": 256,\n",
    "    \"relu3_1\": 256,\n",
    "    \"conv3_2\": 256,\n",
    "    \"relu3_2\": 256,\n",
    "    \"conv3_3\": 256,\n",
    "    \"relu3_3\": 256,\n",
    "    \"pool3\": 256,\n",
    "    \"conv4_1\": 512,\n",
    "    \"relu4_1\": 512,\n",
    "    \"conv4_2\": 512,\n",
    "    \"relu4_2\": 512,\n",
    "    \"conv4_3\": 512,\n",
    "    \"relu4_3\": 512,\n",
    "    \"pool4\": 512,\n",
    "    \"conv5_1\": 512,\n",
    "    \"relu5_1\": 512,\n",
    "    \"conv5_2\": 512,\n",
    "    \"relu5_2\": 512,\n",
    "    \"conv5_3\": 512,\n",
    "    \"relu5_3\": 512,\n",
    "    \"pool5\": 512,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdapLayers(nn.Module):\n",
    "    \"\"\"Small adaptation layers.\"\"\"\n",
    "    def __init__(self, hypercolumn_layers: List[str], output_dim: int = 128):\n",
    "        super(AdapLayers, self).__init__()\n",
    "        self.layers = []\n",
    "        channel_sizes = [vgg16_layers[name] for name in hypercolumn_layers]\n",
    "        for i, l in enumerate(channel_sizes):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv2d(l, 64, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, output_dim, kernel_size=5, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(output_dim),\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "            self.add_module(\"adap_layer_{}\".format(i), layer)\n",
    "\n",
    "    def forward(self, features: List[torch.tensor]):\n",
    "        for i, _ in enumerate(features):\n",
    "            features[i] = getattr(self, \"adap_layer_{}\".format(i))(features[i])\n",
    "        return features\n",
    "\n",
    "class S2DNet(nn.Module):\n",
    "    def __init__(self, device: torch.device, hypercolumn_layers: List[str], checkpoint_path: str = None):\n",
    "        super(S2DNet, self).__init__()\n",
    "        self._device = device\n",
    "        self._hypercolumn_layers = hypercolumn_layers\n",
    "        vgg16 = models.vgg16(pretrained=False)\n",
    "        self.encoder = nn.Sequential(*list(vgg16.features.children())[:-2]).to(device)\n",
    "        self.adaptation_layers = AdapLayers(hypercolumn_layers).to(device)\n",
    "        self.layer_indices = {name: idx for idx, name in enumerate(vgg16_layers) if name in hypercolumn_layers}\n",
    "        \n",
    "        if checkpoint_path:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            self.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "        self.dense_sift_descriptor = DenseSIFTDescriptor(\n",
    "            num_ang_bins=8,\n",
    "            num_spatial_bins=4,\n",
    "            spatial_bin_size=4,\n",
    "            clipval=0.2,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, image_tensor: torch.FloatTensor):\n",
    "        feature_maps = []\n",
    "        x = image_tensor.to(self._device)\n",
    "        for idx, layer in enumerate(self.encoder):\n",
    "            x = layer(x)\n",
    "            layer_name = list(vgg16_layers.keys())[idx]\n",
    "            if layer_name in self._hypercolumn_layers:\n",
    "                feature_maps.append(x)\n",
    "        adapted_features = self.adaptation_layers(feature_maps)\n",
    "\n",
    "\n",
    "        dense_descriptors = self.dense_sift_descriptor(adapted_features)\n",
    "        print(f\"Dense descriptors type and shape: {type(dense_descriptors)}, {dense_descriptors.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "        keypoints, descriptors = self.extract_keypoints_and_descriptors(dense_descriptors)\n",
    "        print(f\"Keypoints type and contents: {type(keypoints)}, {keypoints}\")\n",
    "        print(f\"Descriptors type and shape: {type(descriptors)}, {descriptors.shape}\")\n",
    "        return keypoints, descriptors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def extract_keypoints_and_descriptors(self, dense_descriptors):\n",
    "    # This is a placeholder for whatever method you use to extract keypoints\n",
    "    # For example, you might use a peak detection in the feature strength across the descriptor maps\n",
    "    # Let's assume a simple thresholding for illustration\n",
    "        strength = torch.norm(dense_descriptors, dim=1)  # Compute strength of features\n",
    "        threshold = strength.mean() + strength.std()\n",
    "        keypoints_mask = strength > threshold\n",
    "        keypoints = keypoints_mask.nonzero(as_tuple=True)  # Get indices of keypoints\n",
    "        descriptors = dense_descriptors[:, keypoints[0], keypoints[1]]  # Extract descriptors for keypoints\n",
    "        return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, device, size=(256, 256), maintain_aspect_ratio=False):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return transform(image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m image_path1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuddha1.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     24\u001b[0m image1 \u001b[38;5;241m=\u001b[39m load_image(image_path1, device)\n\u001b[1;32m---> 25\u001b[0m keypoints1, descriptors1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust based on actual model output\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput type:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(keypoints1))\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput contents:\u001b[39m\u001b[38;5;124m\"\u001b[39m, keypoints1)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 56\u001b[0m, in \u001b[0;36mS2DNet.forward\u001b[1;34m(self, image_tensor)\u001b[0m\n\u001b[0;32m     52\u001b[0m         feature_maps\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m     53\u001b[0m adapted_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madaptation_layers(feature_maps)\n\u001b[1;32m---> 56\u001b[0m dense_descriptors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense_sift_descriptor\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapted_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDense descriptors type and shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dense_descriptors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_descriptors\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     61\u001b[0m keypoints, descriptors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_keypoints_and_descriptors(dense_descriptors)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\kornia\\feature\\siftdesc.py:266\u001b[0m, in \u001b[0;36mDenseSIFTDescriptor.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 266\u001b[0m     \u001b[43mKORNIA_CHECK_SHAPE\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mH\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mW\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m     B, CH, W, H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbin_pooling_kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbin_pooling_kernel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\kornia\\core\\check.py:62\u001b[0m, in \u001b[0;36mKORNIA_CHECK_SHAPE\u001b[1;34m(x, shape, raises)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     shape_to_check \u001b[38;5;241m=\u001b[39m shape\n\u001b[1;32m---> 62\u001b[0m     x_shape_to_check \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x_shape_to_check) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(shape_to_check):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raises:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to visualize keypoints on an image\n",
    "def visualize_keypoints(image_tensor, keypoints):\n",
    "    # Convert CHW to HWC and normalize\n",
    "    image_np = image_tensor.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    image_np = (image_np * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])).clip(0, 1)\n",
    "    plt.imshow(image_np)\n",
    "    # Ensure keypoints are in (x, y) format for plotting\n",
    "    if keypoints.dim() > 2:\n",
    "        keypoints = keypoints.squeeze()\n",
    "    plt.scatter(keypoints[:, 1], keypoints[:, 0], s=10, color='red', marker='.')\n",
    "    plt.title(\"Visualized Keypoints\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load model and image\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = S2DNet(device, [\"conv1_2\", \"conv3_3\", \"conv5_3\"], 's2dnet_weights.pth')\n",
    "model.eval()\n",
    "\n",
    "# try:\n",
    "image_path1 = 'buddha1.jpeg'\n",
    "image1 = load_image(image_path1, device)\n",
    "keypoints1, descriptors1 = model(image1)  # Adjust based on actual model output\n",
    "print(\"Output type:\", type(keypoints1))\n",
    "print(\"Output contents:\", keypoints1)\n",
    "\n",
    "#     print(\"Keypoints from Image 1:\", keypoints1.size())\n",
    "#     visualize_keypoints(image1, keypoints1)\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity scores: tensor([[0.4755]], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "Best matches: tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def match_features(features1, features2):\n",
    "    # Flatten the C, H, and W dimensions so each feature map is a single vector\n",
    "    f1_flat = features1.view(features1.shape[0], -1)\n",
    "    f2_flat = features2.view(features2.shape[0], -1)\n",
    "    \n",
    "    # Normalize these vectors to have a unit norm\n",
    "    f1_norm = F.normalize(f1_flat, p=2, dim=1)\n",
    "    f2_norm = F.normalize(f2_flat, p=2, dim=1)\n",
    "    \n",
    "    # Use PyTorch's cosine similarity function across the batch dimension\n",
    "    similarity = F.cosine_similarity(f1_norm.unsqueeze(1), f2_norm.unsqueeze(0), dim=2)\n",
    "    \n",
    "    # Find the best matches for each feature in image1\n",
    "    max_similarity, best_matches = torch.max(similarity, dim=1)\n",
    "\n",
    "    print(\"Similarity scores:\", similarity)  # Debug: print similarity scores to check behavior\n",
    "    return best_matches\n",
    "\n",
    "# Assuming features1 and features2 are the feature tensors from your model\n",
    "best_matches = match_features(features1[0], features2[0])\n",
    "print(\"Best matches:\", best_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# def flatten_features(feature_maps):\n",
    "#     batch_size, channels, height, width = feature_maps.size()\n",
    "#     return feature_maps.view(batch_size, channels, height * width).permute(0, 2, 1).reshape(-1, channels)\n",
    "\n",
    "# # Function to compute cosine similarity\n",
    "# def cosine_similarity(feat1, feat2, batch_size=128):\n",
    "#     feat1_norm = torch.nn.functional.normalize(feat1, p=2, dim=1)\n",
    "#     feat2_norm = torch.nn.functional.normalize(feat2, p=2, dim=1)\n",
    "#     num_rows = feat1_norm.shape[0]\n",
    "#     similarities = []\n",
    "    \n",
    "#     for start in range(0, num_rows, batch_size):\n",
    "#         end = min(start + batch_size, num_rows)\n",
    "#         similarities.append(torch.mm(feat1_norm[start:end], feat2_norm.t()))\n",
    "    \n",
    "#     return torch.cat(similarities, dim=0)\n",
    "\n",
    "\n",
    "# # Flatten the features from both images\n",
    "# features1_flat = flatten_features(features1[0])  # Assuming features1 is a list of tensors\n",
    "# features2_flat = flatten_features(features2[0])\n",
    "\n",
    "# # Calculate similarities\n",
    "# similarities = cosine_similarity(features1_flat, features2_flat)\n",
    "\n",
    "# # Find the best matches\n",
    "# top_matches = torch.topk(similarities, k=1, dim=1)[1].squeeze()\n",
    "\n",
    "# # Load images for visualization\n",
    "# img1 = cv2.imread(image_path1)\n",
    "# img2 = cv2.imread(image_path2)\n",
    "\n",
    "# # Assuming the feature maps are of size (1, C, H, W)\n",
    "# h1, w1 = features1[0].size(2), features1[0].size(3)\n",
    "# h2, w2 = features2[0].size(2), features2[0].size(3)\n",
    "\n",
    "# # Create keypoints for each feature in both images\n",
    "# keypoints1 = [cv2.KeyPoint(x=float(x % w1), y=float(x // w1), _size=1) for x in range(h1 * w1)]\n",
    "# keypoints2 = [cv2.KeyPoint(x=float(x % w2), y=float(x // w2), _size=1) for x in range(h2 * w2)]\n",
    "\n",
    "# # Create matches using the indices found in top_matches\n",
    "# matches = [cv2.DMatch(_queryIdx=int(i), _trainIdx=int(top_matches[i]), _distance=1.0 - float(similarities[i, top_matches[i]])) for i in range(h1 * w1)]\n",
    "\n",
    "# # Draw matches\n",
    "# matched_image = cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# # Display the result\n",
    "# cv2.imshow('Feature Matches', matched_image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms.functional as TF\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define a common size, for example, the smallest dimensions in your list\n",
    "# common_size = (64, 64)  # Adjust as necessary\n",
    "\n",
    "# # Resize tensors\n",
    "# resized_tensors = [TF.resize(t, common_size) if t.size()[2:] != common_size else t for t in features]\n",
    "\n",
    "# # Stack the resized tensors\n",
    "# features_tensor = torch.stack(resized_tensors)\n",
    "\n",
    "# # Move the tensor to CPU, detach it from the graph, and convert to numpy\n",
    "# output_numpy = features_tensor.cpu().detach().numpy()\n",
    "\n",
    "# # Print shapes to debug\n",
    "# print(\"Shape of features_tensor:\", features_tensor.shape)\n",
    "# print(\"Shape of the selected image slice:\", output_numpy[0, 0, 0, :, :].shape)\n",
    "\n",
    "# # Select the first tensor, first channel, first feature map for visualization\n",
    "# image = output_numpy[0, 0, 0, :, :]  # Ensure this index points to a single (64, 64) feature map\n",
    "\n",
    "# plt.imshow(image, cmap='gray')  # Ensure image is a 2D array\n",
    "# plt.colorbar()\n",
    "# plt.title('Visualization of the Resized Output Tensor')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
